{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed301aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import statistics\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import load_vectors_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89ddd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets = 4887\n",
    "mechanism_id = {\n",
    "        'absurd': 1,\n",
    "        'analogy': 2,\n",
    "        'embarrassment': 3,\n",
    "        'exaggeration': 4,\n",
    "        'insults': 5,\n",
    "        'irony': 6,\n",
    "        'misunderstanding': 7,\n",
    "        'parody': 8,\n",
    "        'reference': 9,\n",
    "        'stereotype': 10,\n",
    "        'unmasking': 11,\n",
    "        'wordplay': 12\n",
    "    }\n",
    "\n",
    "model_functions = ['elu', 'relu', 'selu','sigmoid', 'tanh']\n",
    "layer_units = [15, 30, 50, 75, 100]\n",
    "epoch = 150\n",
    "\n",
    "unwanted_chars = ['!', ',', '\"', '-', '...','–','XD', 'xD', '¿', '?', '—', '\\n', \"#\", '¡', ':', \"“\", '.', '(', ')']\n",
    "unwanted_chars.extend([\"¬¬\", \"\\('.')/\", \"*\", '\\n', '»', '\\x97', '\\x85'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664b9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "mechanisms = []\n",
    "words_per_tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb8f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"sources/haha_mechanism_target_train.csv\")\n",
    "\n",
    "train_data.drop(\"id\", axis=1, inplace=True)\n",
    "train_data.drop(\"target\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfb9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_tweet(tweet):\n",
    "    for char in unwanted_chars:\n",
    "        tweet = tweet.replace(char, ' ')\n",
    "    tweet = re.sub('@\\w*', '', tweet) #remove user references\n",
    "    tweet = re.sub('\\$', '$ ', tweet) #split prices chars\n",
    "    tweet = tweet.split(\" \")\n",
    "    tweet = [token for token in tweet if token != ''] #remove unwanted spaces\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5dc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global tweets, mechanisms\n",
    "for idx, row in train_data.iterrows():\n",
    "    mechanisms.append(mechanism_id.get(train_data.loc[idx, \"mechanism\"]))\n",
    "    \n",
    "    sanitized_tweet = sanitize_tweet(train_data.loc[idx, \"text\"])\n",
    "    tweets.append(sanitized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3e042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_word(word_embedding, word):\n",
    "    if word in word_embedding:\n",
    "        return word_embedding[word]\n",
    "    elif word.capitalize() in word_embedding:\n",
    "        return word_embedding[word.capitalize()]\n",
    "    elif word.lower() in word_embedding:\n",
    "        return word_embedding[word.lower()]\n",
    "    elif word.upper() in word_embedding:\n",
    "        return word_embedding[word.upper()]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_embedding_matrix(word_embedding):\n",
    "    word_embedding_words = list(word_embedding.vocab.keys())\n",
    "    word_embedding_word_counter = len(word_embedding_words)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=word_embedding_word_counter, filters='',\n",
    "                                                      lower=False, split=' ', char_level=False, oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(word_embedding_words)\n",
    "    embeddings_word_index = tokenizer.word_index\n",
    "\n",
    "    embedding_matrix = np.zeros((word_embedding_word_counter + 2, 300))\n",
    "    for word, index in embeddings_word_index.items():\n",
    "        if index != 1:\n",
    "            embedding_vector = get_vector_word(word_embedding, word)\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab1ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = load_vectors_300.load(\"emb39-word2vec\")\n",
    "word_embedding = word_embedding.wv\n",
    "word_embedding_matrix, tokenizer = generate_embedding_matrix(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a5aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "num_words_list = list(map(lambda x: len(x), tweets))\n",
    "median_words_tweet = statistics.median(num_words_list)\n",
    "print(median_words_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae1628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "indexes_list = tokenizer.texts_to_sequences(tweets)\n",
    "indexes_list = pad_sequences(indexes_list, maxlen=median_words_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955da7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(indexes_list, mechanisms, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763a75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, verbose=0, mode=\"auto\", restore_best_weights= True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd22ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04379441 -0.04954839 -0.06157952 ... -0.0709765   0.06606881\n",
      "   0.02376981]\n",
      " ...\n",
      " [ 0.05253253 -0.01243497 -0.0963236  ... -0.05829482  0.04032185\n",
      "   0.00046087]\n",
      " [ 0.03058515 -0.07465189  0.00142497 ... -0.06569502  0.01332951\n",
      "  -0.04353482]\n",
      " [ 0.02943384  0.03231565  0.04918109 ... -0.13639031 -0.00750405\n",
      "   0.03460548]]\n"
     ]
    }
   ],
   "source": [
    "global word_embedding_matrix\n",
    "print(word_embedding_matrix)\n",
    "for activation_function in model_functions:\n",
    "    for recurrent_function in model_functions:\n",
    "        gru_gru_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Embedding(input_dim = word_embedding_matrix.shape[0], output_dim = word_embedding_matrix.shape[1],\n",
    "                                 input_length = 1, weights = [word_embedding_matrix], trainable = False, mask_zero = True),\n",
    "            layers.GRU(units = 1, dropout = 0.1, recurrent_dropout = 0.2, activation=activation_function, return_sequences=True),\n",
    "                    layers.GRU(units = 1, dropout = 0.1, recurrent_dropout = 0.3, activation=activation_function),\n",
    "                    layers.Dense(12, activation=activation_function)\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_function in model_functions:\n",
    "    for recurrent_function in model_functions:\n",
    "        for recursive_layers_architecture in recursive_layers_architectures:\n",
    "            gru_lstm_model = keras.Sequential(\n",
    "                [\n",
    "                    layers.Embedding(input_dim = embedding_matix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                                 input_length = 1, weights = [], trainable = False, mask_zero = True),\n",
    "                    layers.GRU(units = 1, dropout = DROPOUT, recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                           kernel_initializer=KERNEL_INITIALIZER1, activation=activation_function, return_sequences=True),\n",
    "                    layers.LSTM(units = 1, dropout = DROPOUT, recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                           kernel_initializer=KERNEL_INITIALIZER1, activation=activation_function),\n",
    "                    layers.Dense(TARGETS, activation=activation_function)\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_function in model_functions:\n",
    "    for recurrent_function in model_functions:\n",
    "        for recursive_layers_architecture in recursive_layers_architectures:\n",
    "            lstm_gru_model = keras.Sequential(\n",
    "                [\n",
    "                    layers.Embedding(input_dim = embedding_matix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                                 input_length = 1, weights = [], trainable = False, mask_zero = True),\n",
    "                    layers.LSTM(units = 1, dropout = DROPOUT, recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                           kernel_initializer=KERNEL_INITIALIZER1, activation=activation_function, return_sequences=True),\n",
    "                    layers.GRU(units = 1, dropout = DROPOUT, recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                           kernel_initializer=KERNEL_INITIALIZER1, activation=activation_function),\n",
    "                    layers.Dense(TARGETS, activation=activation_function)\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_function in model_functions:\n",
    "    for recurrent_function in model_functions:\n",
    "        for recursive_layers_architecture in recursive_layers_architectures:\n",
    "            lstm_lstm_model = keras.Sequential(\n",
    "                [\n",
    "                    layers.Embedding(input_dim = embedding_matix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                                 input_length = 1, weights = [], trainable = False, mask_zero = True),\n",
    "                    layers.LSTM(units = 1, dropout = DROPOUT, recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                           kernel_initializer=KERNEL_INITIALIZER1, activation=activation_function, return_sequences=True),\n",
    "                    layers.LSTM(units = 1, dropout = DROPOUT, recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                           kernel_initializer=KERNEL_INITIALIZER1, activation=activation_function),\n",
    "                    layers.Dense(TARGETS, activation=activation_function)\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c79f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
